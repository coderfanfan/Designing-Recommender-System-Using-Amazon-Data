{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import gym\n",
    "from value_iteration import value_iteration\n",
    "from policy_iteration import policy_iteration\n",
    "from Q_learning import q_learning\n",
    "from frozenlake import FrozenLakeEnv\n",
    "from plotting import *\n",
    "import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '5x5 FrozenLake'\n",
    "env = FrozenLakeEnv(map_name=\"5x5\", is_slippery=False, hole_reward=True)\n",
    "#name = '10x10 FrozenLake'\n",
    "#env = FrozenLakeEnv(map_name=\"10x10\", is_slippery=False, hole_reward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration and Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(9001)\n",
    "policy_v, _, V_v, converge_iter_v, time_iter_v = value_iteration(env, theta=0.0001, discount_factor=0.99, max_iter=50, early_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(9001)\n",
    "policy_p, _, V_p, converge_iter_p, time_iter_p = policy_iteration(env, discount_factor=0.99, max_iter=50, early_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(converge_iter_v, label='Value Iteration')\n",
    "plt.plot(converge_iter_p, label='Policy Iteration')\n",
    "plt.legend()\n",
    "plt.title('%s Convergence Value'%name)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Convergence value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_iter_v, label='Value Iteration')\n",
    "plt.plot(time_iter_p, label='Policy Iteration')\n",
    "plt.legend()\n",
    "plt.title('%s Clock Time'%name)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('clock time of each iteration (ms) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_map(\"5x5 FrozenLake\", policy_v, env.desc, env.colors(), env.directions())\n",
    "#plot_policy_map(\"Value Iteration Converged Policy\", policy_v, env.desc, env.colors(), env.directions())\n",
    "#plot_policy_map(\"Policy Iteration Converged Policy\", policy_p, env.desc, env.colors(), env.directions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_value_map(\"Value Iteration Optimal Value\", V_v, env.desc, env.colors())\n",
    "plot_value_map(\"Policy Iteration Optimal Value\", V_p, env.desc, env.colors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(9001)\n",
    "#policy_qs, V_qs, converge_iter_qs, time_iter_qs, reward_iter_qs = q_learning(env, num_episodes=50000, next_action='simulated_annealing', discount_factor=0.99, decay=0.99999, max_iter=1000)\n",
    "e=0.8\n",
    "l=0.9\n",
    "policy_qe, V_qe, converge_iter_qe, time_iter_qe, reward_iter_qe = q_learning(env, num_episodes=500000, next_action='epsilon_greedy', discount_factor=0.99, alpha=l, epsilon=e, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(converge_iter_qe, label='Q-learning epsilon greedy')\n",
    "plt.legend()\n",
    "plt.title('%s Convergence Value'%name)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Convergence value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_policy_map(\"\", policy_qe, env.desc, env.colors(), env.directions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_value_map(\"Q-learning Values after 10,000 episodes\", V_qe, env.desc, env.colors())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
